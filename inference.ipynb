{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import flash_attn\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterable,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Generator,\n",
    "    List,\n",
    "    NamedTuple,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "# Ensure BAGEL specific modules are in your PYTHONPATH or current working directory structure\n",
    "# These are assumed to be part of the BAGEL codebase you have in /workspace/BAGEL/\n",
    "from data.transforms import ImageTransform\n",
    "from data.data_utils import pil_img2rgb, add_special_tokens\n",
    "from modeling.bagel import (\n",
    "    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel\n",
    ")\n",
    "from modeling.qwen2 import Qwen2Tokenizer\n",
    "from modeling.bagel.qwen2_navit import NaiveCache\n",
    "from modeling.autoencoder import load_ae\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True # Can impact performance\n",
    "# torch.backends.cudnn.benchmark = False   # Can impact performance\n",
    "print(f\"Seeds set to {seed}\")\n",
    "\n",
    "print(\"All initial imports successful.\")\n",
    "\n",
    "model_path = \"downloads\"  # Path relative to /workspace. Should be /workspace/downloads/\n",
    "if not os.path.exists(model_path) or not os.path.exists(os.path.join(model_path, \"llm_config.json\")):\n",
    "    print(f\"ERROR: Model path '{model_path}' not found or doesn't contain expected files.\")\n",
    "    print(\"Please download BAGEL model files from https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT and place them in /workspace/downloads/\")\n",
    "    # You might want to stop execution here if files are missing\n",
    "    # raise FileNotFoundError(\"Model files not found in 'downloads' directory\")\n",
    "else:\n",
    "    print(f\"Using model files from: {os.path.abspath(model_path)}\")\n",
    "\n",
    "# LLM config preparing\n",
    "llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
    "llm_config.qk_norm = True\n",
    "llm_config.tie_word_embeddings = False\n",
    "llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "\n",
    "# ViT config preparing\n",
    "vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
    "vit_config.rope = False\n",
    "vit_config.num_hidden_layers = vit_config.num_hidden_layers - 1\n",
    "\n",
    "# VAE loading\n",
    "vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
    "\n",
    "# Bagel config preparing\n",
    "config = BagelConfig(\n",
    "    visual_gen=True,\n",
    "    visual_und=True,\n",
    "    llm_config=llm_config, \n",
    "    vit_config=vit_config,\n",
    "    vae_config=vae_config,\n",
    "    vit_max_num_patch_per_side=70,\n",
    "    connector_act='gelu_pytorch_tanh',\n",
    "    latent_patch_size=2,\n",
    "    max_latent_size=64,\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    language_model = Qwen2ForCausalLM(llm_config)\n",
    "    vit_model      = SiglipVisionModel(vit_config)\n",
    "    model          = Bagel(language_model, vit_model, config)\n",
    "    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
    "\n",
    "# Tokenizer Preparing\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "\n",
    "# Image Transform Preparing\n",
    "vae_transform = ImageTransform(1024, 512, 16)\n",
    "vit_transform = ImageTransform(980, 224, 14)\n",
    "\n",
    "print(\"Model components initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "if num_gpus == 0:\n",
    "    raise EnvironmentError(\"No GPUs detected by PyTorch. Cannot proceed with GPU mapping.\")\n",
    "\n",
    "# llm_config and model (empty shell) must be defined before this cell\n",
    "num_llm_layers = llm_config.num_hidden_layers\n",
    "print(f\"Number of LLM layers: {num_llm_layers}\")\n",
    "\n",
    "device_map = {}\n",
    "\n",
    "# --- LLM Core Distribution ---\n",
    "llm_embed_gpu = 0\n",
    "device_map[\"language_model.model.embed_tokens\"] = llm_embed_gpu\n",
    "\n",
    "# Distribute LLM layers\n",
    "# For 2 GPUs: layers 0 to N/2-1 on GPU 0, N/2 to N-1 on GPU 1\n",
    "# For N GPUs: round-robin or block-wise\n",
    "llm_layer_split_points = [ (i * num_llm_layers) // num_gpus for i in range(num_gpus + 1) ]\n",
    "current_gpu_for_llm = 0\n",
    "for i in range(num_llm_layers):\n",
    "    if num_gpus > 1 and i >= llm_layer_split_points[current_gpu_for_llm + 1]:\n",
    "        current_gpu_for_llm = min(current_gpu_for_llm + 1, num_gpus - 1)\n",
    "    device_map[f\"language_model.model.layers.{i}\"] = current_gpu_for_llm\n",
    "\n",
    "final_llm_components_gpu = current_gpu_for_llm # GPU where last LLM layer landed\n",
    "device_map[\"language_model.model.norm\"] = final_llm_components_gpu\n",
    "device_map[\"language_model.lm_head\"] = final_llm_components_gpu\n",
    "if hasattr(model.language_model.model, 'norm_moe_gen'):\n",
    "    device_map[\"language_model.model.norm_moe_gen\"] = final_llm_components_gpu\n",
    "\n",
    "# --- Vision Components ---\n",
    "# vit_model itself can be large, place it on a GPU (e.g., the last one if multi-GPU)\n",
    "vit_processing_gpu = (num_gpus - 1) if num_gpus > 1 else 0\n",
    "device_map[\"vit_model\"] = vit_processing_gpu\n",
    "\n",
    "# **CRITICAL FIX:** Place vit_pos_embed and connector on the GPU where their\n",
    "# output will be combined with other embeddings for the LLM.\n",
    "# This is typically the GPU where the LLM's main processing begins (where embed_tokens are).\n",
    "interaction_gpu = llm_embed_gpu # cuda:0 in this setup\n",
    "\n",
    "device_map[\"vit_pos_embed\"] = interaction_gpu # MOVED TO GPU 0\n",
    "device_map[\"connector\"] = interaction_gpu     # Kept on GPU 0\n",
    "\n",
    "# --- Other Bagel-specific components ---\n",
    "device_map[\"time_embedder\"] = llm_embed_gpu\n",
    "device_map[\"latent_pos_embed\"] = llm_embed_gpu\n",
    "device_map[\"vae2llm\"] = llm_embed_gpu\n",
    "device_map[\"llm2vae\"] = final_llm_components_gpu\n",
    "\n",
    "\n",
    "print(\"Constructed manual device_map:\", device_map)\n",
    "\n",
    "# --- The rest of your loading code (offload_folder, load_checkpoint_and_dispatch) ---\n",
    "offload_folder = \"/tmp/offload_bagel\"\n",
    "disk_offload_active = any(v == 'disk' for v in device_map.values())\n",
    "if disk_offload_active:\n",
    "    if not os.path.exists(offload_folder):\n",
    "        os.makedirs(offload_folder, exist_ok=True)\n",
    "    print(f\"Created offload folder: {offload_folder}\")\n",
    "    print(f\"Disk offloading is active. Using offload folder: {offload_folder}\")\n",
    "else:\n",
    "    print(\"No disk offloading specified in manual device_map.\")\n",
    "\n",
    "# model_path = \"downloads\" # Ensure model_path is defined\n",
    "\n",
    "print(\"Attempting to load checkpoint with manual device map...\")\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n",
    "    device_map=device_map,\n",
    "    offload_buffers=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    force_hooks=True,\n",
    "    offload_folder=offload_folder,\n",
    "    max_memory={g: \"44GiB\" for g in range(num_gpus)}\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "print('Model loaded and dispatched successfully.')\n",
    "\n",
    "offload_folder = \"/tmp/offload_bagel\"\n",
    "disk_offload_active = any(v == 'disk' for v in device_map.values())\n",
    "if disk_offload_active:\n",
    "    if not os.path.exists(offload_folder):\n",
    "        os.makedirs(offload_folder, exist_ok=True)\n",
    "        print(f\"Created offload folder: {offload_folder}\")\n",
    "    print(f\"Disk offloading is active. Offload folder: {offload_folder}\")\n",
    "else:\n",
    "    print(\"No disk offloading specified in manual device_map.\")\n",
    "\n",
    "# Ensure model_path is defined\n",
    "# model_path = \"downloads\"\n",
    "\n",
    "print(\"Attempting to load checkpoint with manual device map...\")\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n",
    "    device_map=device_map,\n",
    "    offload_buffers=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    force_hooks=True,\n",
    "    offload_folder=offload_folder,\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "print('Model loaded and dispatched.')\n",
    "# ----- END MODIFIED SECTION -----\n",
    "\n",
    "from inferencer import InterleaveInferencer\n",
    "\n",
    "inferencer = InterleaveInferencer(\n",
    "    model=model, \n",
    "    vae_model=vae_model, \n",
    "    tokenizer=tokenizer, \n",
    "    vae_transform=vae_transform, \n",
    "    vit_transform=vit_transform, \n",
    "    new_token_ids=new_token_ids\n",
    ")\n",
    "print(\"Inferencer prepared.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About Inference Hyperparameters:**\n",
    "- **`cfg_text_scale`:** Controls how strongly the model follows the text prompt. `1.0` disables text guidance. Typical range: `4.0–8.0`.\n",
    "- **`cfg_image_scale`:** Controls how much the model preserves input image details. `1.0` disables image guidance. Typical range: `1.0–2.0`.\n",
    "- **`cfg_interval`:** Fraction of denoising steps where CFG is applied. Later steps can skip CFG to reduce computation. Typical: `[0.4, 1.0]`.\n",
    "- **`timestep_shift`:** Shifts the distribution of denoising steps. Higher values allocate more steps at the start (affects layout); lower values allocate more at the end (improves details).\n",
    "- **`num_timesteps`:** Total denoising steps. Typical: `50`.\n",
    "- **`cfg_renorm_min`:** Minimum value for CFG-Renorm. `1.0` disables renorm. Typical: `0`.\n",
    "- **`cfg_renorm_type`:** CFG-Renorm method:  \n",
    "  - `global`: Normalize over all tokens and channels (default for T2I).\n",
    "  - `channel`: Normalize across channels for each token.\n",
    "  - `text_channel`: Like `channel`, but only applies to text condition (good for editing, may cause blur).\n",
    "- **If edited images appear blurry, try `global` CFG-Renorm, decrease `cfg_renorm_min` or decrease `cfg_scale`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper=dict(\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=1.0,\n",
    "    cfg_interval=[0.4, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=1.0,\n",
    "    cfg_renorm_type=\"global\",\n",
    ")\n",
    "\n",
    "prompt = \"A colorful friendly crhulhu is smiling and kissing a nice emo girl, cartoon\"\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print('-' * 10)\n",
    "try:\n",
    "    output_dict = inferencer(text=prompt, **inference_hyper)\n",
    "    if 'image' in output_dict and output_dict['image'] is not None:\n",
    "        print(\"Image generated successfully.\")\n",
    "        display(output_dict['image'])\n",
    "    else:\n",
    "        print(\"Inference completed, but no image found in output_dict or image is None.\")\n",
    "        print(\"Output dictionary:\", output_dict)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image Generation with Think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper_think=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=1.0,\n",
    "    cfg_interval=[0.4, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=1.0,\n",
    "    cfg_renorm_type=\"global\",\n",
    ")\n",
    "\n",
    "prompt_think = 'a car made of small cars'\n",
    "\n",
    "print(\"Prompt for think & gen:\", prompt_think)\n",
    "print('-' * 10)\n",
    "try:\n",
    "    output_dict_think = inferencer(text=prompt_think, think=True, **inference_hyper_think)\n",
    "    print(\"Generated text (thought process):\", output_dict_think.get('text', 'N/A'))\n",
    "    if 'image' in output_dict_think and output_dict_think['image'] is not None:\n",
    "        print(\"Image generated successfully with think.\")\n",
    "        display(output_dict_think['image'])\n",
    "    else:\n",
    "        print(\"Inference with think completed, but no image found in output_dict or image is None.\")\n",
    "        print(\"Output dictionary (think):\", output_dict_think)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during inference with think: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper_edit=dict(\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=1.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    ")\n",
    "\n",
    "# Ensure you have a 'test_images' directory with 'women.jpg'\n",
    "edit_image_path = 'test_images/women.jpg'\n",
    "prompt_edit = 'She boards a modern subway, quietly reading a folded newspaper, wearing the same clothes.'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(edit_image_path):\n",
    "        image_edit_input = Image.open(edit_image_path)\n",
    "        print(\"Input image for editing:\")\n",
    "        display(image_edit_input)\n",
    "        print(\"Prompt for editing:\", prompt_edit)\n",
    "        print('-'*10)\n",
    "        output_dict_edit = inferencer(image=image_edit_input, text=prompt_edit, **inference_hyper_edit)\n",
    "        if 'image' in output_dict_edit and output_dict_edit['image'] is not None:\n",
    "            print(\"Edited image generated successfully.\")\n",
    "            display(output_dict_edit['image'])\n",
    "        else:\n",
    "            print(\"Editing inference completed, but no image found in output_dict or image is None.\")\n",
    "            print(\"Output dictionary (edit):\", output_dict_edit)\n",
    "    else:\n",
    "        print(f\"ERROR: Edit image path not found: {edit_image_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during editing inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Edit with Think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper_edit_think=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    ")\n",
    "\n",
    "# Ensure you have a 'test_images' directory with 'octupusy.jpg'\n",
    "edit_think_image_path = 'test_images/octupusy.jpg'\n",
    "prompt_edit_think = 'Could you display the sculpture that takes after this design?'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(edit_think_image_path):\n",
    "        image_edit_think_input = Image.open(edit_think_image_path)\n",
    "        print(\"Input image for edit with think:\")\n",
    "        display(image_edit_think_input)\n",
    "        print(\"Prompt for edit with think:\", prompt_edit_think)\n",
    "        print('-'*10)\n",
    "        output_dict_edit_think = inferencer(image=image_edit_think_input, text=prompt_edit_think, think=True, **inference_hyper_edit_think)\n",
    "        print(\"Generated text (edit with think):\", output_dict_edit_think.get('text', 'N/A'))\n",
    "        if 'image' in output_dict_edit_think and output_dict_edit_think['image'] is not None:\n",
    "            print(\"Edited image with think generated successfully.\")\n",
    "            display(output_dict_edit_think['image'])\n",
    "        else:\n",
    "            print(\"Edit with think inference completed, but no image found in output_dict or image is None.\")\n",
    "            print(\"Output dictionary (edit with think):\", output_dict_edit_think)\n",
    "    else:\n",
    "        print(f\"ERROR: Edit with think image path not found: {edit_think_image_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during edit with think inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper_und=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    text_temperature=0.3,\n",
    ")\n",
    "# Ensure you have a 'test_images' directory with 'meme.jpg'\n",
    "und_image_path = '/workspace/BAGEL/1.png'\n",
    "prompt_und = \"Fully transcribe all musical symbols\"\n",
    "\n",
    "try:\n",
    "    if os.path.exists(und_image_path):\n",
    "        image_und_input = Image.open(und_image_path)\n",
    "        print(\"Input image for understanding:\")\n",
    "        display(image_und_input)\n",
    "        print(\"Prompt for understanding:\", prompt_und)\n",
    "        print('-'*10)\n",
    "        output_dict_und = inferencer(image=image_und_input, text=prompt_und, understanding_output=True, **inference_hyper_und)\n",
    "        print(\"Generated text (understanding):\", output_dict_und.get('text', 'N/A'))\n",
    "    else:\n",
    "        print(f\"ERROR: Understanding image path not found: {und_image_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during understanding inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
